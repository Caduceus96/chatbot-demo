{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "NUMBER_SAMPLES = 1000 # ----- to test on a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Note: need to prepare datasets using https://github.com/rkadlec/ubuntu-ranking-dataset-creator.git to generate\n",
    "# ----- the train.csv, test.csv files used in this script\n",
    "\n",
    "# ----- load datasets \n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATA_PATH, \"test.csv\"))\n",
    "\n",
    "if NUMBER_SAMPLES is not None:\n",
    "    train_df = train_df.loc[np.random.choice(range(train_df.shape[0]), NUMBER_SAMPLES), ]\n",
    "    test_df = test_df.loc[np.random.choice(range(train_df.shape[0]), NUMBER_SAMPLES/100), ]\n",
    "\n",
    "train_data_response = list()\n",
    "test_data_response=list()\n",
    "valid_data_response=list()\n",
    "\n",
    "# ----- fit tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df.Context)\n",
    "\n",
    "# ----- preprocess train data context, encode as sequence of ints, pad sequences to fixed length\n",
    "\n",
    "train_data_context = keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenizer.texts_to_sequences(train_df.Context.astype(str)), \n",
    "    maxlen=160, \n",
    "    dtype='int32', \n",
    "    padding='post', \n",
    "    truncating='post', \n",
    "    value=0.0\n",
    ")\n",
    "\n",
    "# ---- needed for model parameters\n",
    "\n",
    "vocab_size = train_data_context.max()+1\n",
    "\n",
    "# ----- preprocess train data response\n",
    "\n",
    "train_data_response = keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenizer.texts_to_sequences(train_df.Utterance.astype(str)),\n",
    "    maxlen=160, \n",
    "    dtype='int32', \n",
    "    padding='post', \n",
    "    truncating='post', \n",
    "    value=0.0\n",
    ")\n",
    "\n",
    "# ----- train data labels\n",
    "\n",
    "train_data_labels = train_df.Label.copy()\n",
    "\n",
    "# ----- preprocess test dataset \n",
    "\n",
    "# ----- we tile the response since we have 9 false responses for each true label, each with same context\n",
    "\n",
    "test_data_context = np.tile(\n",
    "    keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenizer.texts_to_sequences(test_df.Context.astype(str)), \n",
    "        maxlen=160, \n",
    "        dtype='int32', \n",
    "        padding='post', \n",
    "        truncating='post',\n",
    "        value=0.0), \n",
    "    (10,1)\n",
    ")\n",
    "\n",
    "# ----- preprocess test data response for true responses\n",
    "\n",
    "test_data_response = keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenizer.texts_to_sequences(test_df[\"Ground Truth Utterance\"].astype(str)), \n",
    "    maxlen=160, \n",
    "    dtype='int32', \n",
    "    padding='post', \n",
    "    truncating='post', \n",
    "    value=0.0\n",
    ")\n",
    "\n",
    "# ----- add in distractor (false responses, sampled randomly from dataset)\n",
    "\n",
    "for r in range(9):\n",
    "    test_data_distractor = tokenizer.texts_to_sequences(test_df[\"Distractor_{}\".format(r)].astype(str))\n",
    "    test_data_distractor = keras.preprocessing.sequence.pad_sequences(\n",
    "        test_data_distractor, \n",
    "        maxlen=160, \n",
    "        dtype='int32', \n",
    "        padding='post', \n",
    "        truncating='post', \n",
    "        value=0.0\n",
    "    )\n",
    "    test_data_response = np.concatenate([test_data_response,test_data_distractor])\n",
    "\n",
    "# ----- test labels  \n",
    "\n",
    "test_data_labels = np.tile(np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]), (test_data_response.shape[0]/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Siamese Dual LSTM for Slot-filling Chatbot\n",
    "\n",
    "# ----- declare custom keras layer\n",
    "\n",
    "class SimilarityLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom \"similarity\" layer that computes inner product of the predicted (r') and real (r) response. See\n",
    "    Lowe et al., Proceedings of the SIGDIAL 2015 Conference, p. 290 for details\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_dim, **kwargs):\n",
    "        self.kernel_dim = kernel_dim\n",
    "        self.result = None\n",
    "        super(SimilarityLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(self.kernel_dim, self.kernel_dim),\n",
    "                                      initializer='truncated_normal',\n",
    "                                      trainable=True)\n",
    "        super(SimilarityLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.result = K.sigmoid(K.dot(inputs[0], K.dot(self.kernel, K.transpose(inputs[1]))))\n",
    "        return self.result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return K.int_shape(self.result)\n",
    "\n",
    "MODEL_DIM = 20    \n",
    "    \n",
    "# ----- input layers, embedding, LSTM unit, following Lowe et. al. 2015, to encode the context and response\n",
    "    \n",
    "context_input = keras.layers.Input(shape=(160,))\n",
    "context_encoding = keras.layers.Embedding(vocab_size, MODEL_DIM, input_length=160)(context_input)\n",
    "context_encoding = keras.layers.LSTM(MODEL_DIM)(context_encoding)\n",
    "\n",
    "response_input = keras.layers.Input(shape=(160,))\n",
    "response_encoding = keras.layers.Embedding(vocab_size, MODEL_DIM, input_length=160)(response_input)\n",
    "response_encoding = keras.layers.LSTM(MODEL_DIM)(response_encoding)\n",
    "\n",
    "# ----- our customer similarity layer, to compute the inner product of predicted and real response\n",
    "\n",
    "predicted_similarity = SimilarityLayer(kernel_dim=MODEL_DIM)([response_encoding, context_encoding])\n",
    "\n",
    "# ----- declare inputs and outputs in dual, \"siamese\" LSTM model\n",
    "\n",
    "dual_lstm_model = keras.models.Model(inputs=[context_input, response_input], outputs=[predicted_similarity])\n",
    "\n",
    "# ----- compile model\n",
    "\n",
    "dual_lstm_model.compile(optimizer=keras.optimizers.Adam(lr=1e-2), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----- fit model\n",
    "\n",
    "dual_lstm_model_history = dual_lstm_model.fit( \n",
    "    batch_size=128,\n",
    "    x=[train_data_context, train_data_response], \n",
    "    y=train_data_labels, \n",
    "    validation_data=([test_data_context, test_data_response], test_data_labels), \n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Bidrectional LSTM + Attention Decoder\n",
    "\n",
    "# ----- custom attention layer from https://github.com/datalogue/keras-attention.git\n",
    "\n",
    "import sys\n",
    "sys.path.append('keras-attention')\n",
    "from models.custom_recurrents import AttentionDecoder\n",
    "from keras.layers.wrappers import Bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- input and Bidirectional Encoding\n",
    "\n",
    "MODEL_DIM = 20\n",
    "\n",
    "context_input = keras.layers.Input(shape=(160,))\n",
    "context_encoding = keras.layers.Embedding(vocab_size, MODEL_DIM, input_length=160)(context_input)\n",
    "context_encoding = Bidirectional(\n",
    "    keras.layers.LSTM(MODEL_DIM, return_sequences=True), merge_mode='concat'\n",
    ")(context_encoding)\n",
    "context_decoding = AttentionDecoder(MODEL_DIM, vocab_size)(context_encoding)\n",
    "\n",
    "# ----- declare model\n",
    "\n",
    "bidirectional_attention_model = keras.models.Model(inputs=context_input, outputs=context_decoding)\n",
    "\n",
    "# ----- compile model \n",
    "\n",
    "bidirectional_attention_model.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----- fit model\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "bidirectional_attention_model_history  = bidirectional_attention_model.fit( \n",
    "    batch_size=128,\n",
    "    x=train_data_context, \n",
    "    y=np_utils.to_categorical(train_data_response, num_classes=vocab_size), \n",
    "    validation_data=(test_data_context, np_utils.to_categorical(test_data_response, num_classes=vocab_size)), \n",
    "    epochs=100\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

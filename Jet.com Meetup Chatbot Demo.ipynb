{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "NUMBER_SAMPLES = 1000 # ----- to test on a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Note: need to prepare datasets using https://github.com/rkadlec/ubuntu-ranking-dataset-creator.git to generate\n",
    "# ----- the train.csv, test.csv files used in this script\n",
    "\n",
    "# ----- load datasets \n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATA_PATH, \"test.csv\"))\n",
    "\n",
    "if NUMBER_SAMPLES is not None:\n",
    "    train_df = train_df.loc[np.random.choice(range(train_df.shape[0]), NUMBER_SAMPLES), ]\n",
    "    test_df = test_df.loc[np.random.choice(range(train_df.shape[0]), NUMBER_SAMPLES/100), ]\n",
    "\n",
    "train_data_response = list()\n",
    "test_data_response=list()\n",
    "valid_data_response=list()\n",
    "\n",
    "# ----- fit tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df.Context)\n",
    "\n",
    "# ----- preprocess train data context, encode as sequence of ints, pad sequences to fixed length\n",
    "\n",
    "train_data_context = keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenizer.texts_to_sequences(train_df.Context.astype(str)), \n",
    "    maxlen=160, \n",
    "    dtype='int32', \n",
    "    padding='post', \n",
    "    truncating='post', \n",
    "    value=0.0\n",
    ")\n",
    "\n",
    "# ---- needed for model parameters\n",
    "\n",
    "vocab_size = train_data_context.max()+1\n",
    "\n",
    "# ----- preprocess train data response\n",
    "\n",
    "train_data_response = keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenizer.texts_to_sequences(train_df.Utterance.astype(str)),\n",
    "    maxlen=160, \n",
    "    dtype='int32', \n",
    "    padding='post', \n",
    "    truncating='post', \n",
    "    value=0.0\n",
    ")\n",
    "\n",
    "# ----- train data labels\n",
    "\n",
    "train_data_labels = train_df.Label.copy()\n",
    "\n",
    "# ----- preprocess test dataset \n",
    "\n",
    "# ----- we tile the response since we have 9 false responses for each true label, each with same context\n",
    "\n",
    "test_data_context = np.tile(\n",
    "    keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenizer.texts_to_sequences(test_df.Context.astype(str)), \n",
    "        maxlen=160, \n",
    "        dtype='int32', \n",
    "        padding='post', \n",
    "        truncating='post',\n",
    "        value=0.0), \n",
    "    (10,1)\n",
    ")\n",
    "\n",
    "# ----- preprocess test data response for true responses\n",
    "\n",
    "test_data_response = keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenizer.texts_to_sequences(test_df[\"Ground Truth Utterance\"].astype(str)), \n",
    "    maxlen=160, \n",
    "    dtype='int32', \n",
    "    padding='post', \n",
    "    truncating='post', \n",
    "    value=0.0\n",
    ")\n",
    "\n",
    "# ----- add in distractor (false responses, sampled randomly from dataset)\n",
    "\n",
    "for r in range(9):\n",
    "    test_data_distractor = tokenizer.texts_to_sequences(test_df[\"Distractor_{}\".format(r)].astype(str))\n",
    "    test_data_distractor = keras.preprocessing.sequence.pad_sequences(\n",
    "        test_data_distractor, \n",
    "        maxlen=160, \n",
    "        dtype='int32', \n",
    "        padding='post', \n",
    "        truncating='post', \n",
    "        value=0.0\n",
    "    )\n",
    "    test_data_response = np.concatenate([test_data_response,test_data_distractor])\n",
    "\n",
    "# ----- test labels  \n",
    "\n",
    "test_data_labels = np.tile(np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]), (test_data_response.shape[0]/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Siamese Dual LSTM for Slot-filling Chatbot\n",
    "\n",
    "# ----- declare custom keras layer\n",
    "\n",
    "class SimilarityLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom \"similarity\" layer that computes inner product of the predicted (r') and real (r) response. See\n",
    "    Lowe et al., Proceedings of the SIGDIAL 2015 Conference, p. 290 for details\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_dim, **kwargs):\n",
    "        self.kernel_dim = kernel_dim\n",
    "        self.result = None\n",
    "        super(SimilarityLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(self.kernel_dim, self.kernel_dim),\n",
    "                                      initializer='truncated_normal',\n",
    "                                      trainable=True)\n",
    "        super(SimilarityLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.result = K.sigmoid(K.dot(inputs[0], K.dot(self.kernel, K.transpose(inputs[1]))))\n",
    "        return self.result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return K.int_shape(self.result)\n",
    "\n",
    "MODEL_DIM = 20    \n",
    "    \n",
    "# ----- input layers, embedding, LSTM unit, following Lowe et. al. 2015, to encode the context and response\n",
    "    \n",
    "context_input = keras.layers.Input(shape=(160,))\n",
    "context_encoding = keras.layers.Embedding(vocab_size, MODEL_DIM, input_length=160)(context_input)\n",
    "context_encoding = keras.layers.LSTM(MODEL_DIM)(context_encoding)\n",
    "\n",
    "response_input = keras.layers.Input(shape=(160,))\n",
    "response_encoding = keras.layers.Embedding(vocab_size, MODEL_DIM, input_length=160)(response_input)\n",
    "response_encoding = keras.layers.LSTM(MODEL_DIM)(response_encoding)\n",
    "\n",
    "# ----- our customer similarity layer, to compute the inner product of predicted and real response\n",
    "\n",
    "predicted_similarity = SimilarityLayer(kernel_dim=MODEL_DIM)([response_encoding, context_encoding])\n",
    "\n",
    "# ----- declare inputs and outputs in dual, \"siamese\" LSTM model\n",
    "\n",
    "dual_lstm_model = keras.models.Model(inputs=[context_input, response_input], outputs=[predicted_similarity])\n",
    "\n",
    "# ----- compile model\n",
    "\n",
    "dual_lstm_model.compile(optimizer=keras.optimizers.Adam(lr=1e-2), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 16s 2ms/step - loss: 0.6932 - val_loss: 0.7040\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6930 - val_loss: 0.6941\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6931 - val_loss: 0.7091\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6929 - val_loss: 0.7033\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6929 - val_loss: 0.7357\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6931 - val_loss: 0.6988\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6928 - val_loss: 0.7176\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6929 - val_loss: 0.7264\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6929 - val_loss: 0.7165\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6928 - val_loss: 0.7286\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6929 - val_loss: 0.7071\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6927 - val_loss: 0.7206\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6928 - val_loss: 0.7054\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6927 - val_loss: 0.7078\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.6926 - val_loss: 0.7219\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6926 - val_loss: 0.7060\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6926 - val_loss: 0.7082\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6926 - val_loss: 0.7240\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6927 - val_loss: 0.7164\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6926 - val_loss: 0.7179\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6926 - val_loss: 0.7144\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6926 - val_loss: 0.7184\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6926 - val_loss: 0.7188\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6925 - val_loss: 0.7221\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6924 - val_loss: 0.7071\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6925 - val_loss: 0.7094\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6926 - val_loss: 0.7313\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6926 - val_loss: 0.7154\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6925 - val_loss: 0.7114\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6926 - val_loss: 0.6916\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.6927 - val_loss: 0.6915\n",
      "Epoch 32/100\n",
      " 9472/10000 [===========================>..] - ETA: 0s - loss: 0.6927"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-722b2820addd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_data_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_response\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----- fit model\n",
    "\n",
    "dual_lstm_model_history = dual_lstm_model.fit( \n",
    "    batch_size=128,\n",
    "    x=[train_data_context, train_data_response], \n",
    "    y=train_data_labels, \n",
    "    validation_data=([test_data_context, test_data_response], test_data_labels), \n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Bidrectional LSTM + Attention Decoder\n",
    "\n",
    "# ----- custom attention layer from https://github.com/datalogue/keras-attention.git\n",
    "\n",
    "import sys\n",
    "sys.path.append('keras-attention')\n",
    "from models.custom_recurrents import AttentionDecoder\n",
    "from keras.layers.wrappers import Bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('inputs shape:', TensorShape([Dimension(None), Dimension(None), Dimension(40)]))\n"
     ]
    }
   ],
   "source": [
    "# ---- input and Bidirectional Encoding\n",
    "\n",
    "MODEL_DIM = 20\n",
    "\n",
    "context_input = keras.layers.Input(shape=(160,))\n",
    "context_encoding = keras.layers.Embedding(vocab_size, MODEL_DIM, input_length=160)(context_input)\n",
    "context_encoding = Bidirectional(\n",
    "    keras.layers.LSTM(MODEL_DIM, return_sequences=True), merge_mode='concat'\n",
    ")(context_encoding)\n",
    "context_decoding = AttentionDecoder(MODEL_DIM, vocab_size)(context_encoding)\n",
    "\n",
    "# ----- declare model\n",
    "\n",
    "bidirectional_attention_model = keras.models.Model(inputs=context_input, outputs=context_decoding)\n",
    "\n",
    "# ----- compile model \n",
    "\n",
    "bidirectional_attention_model.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/100\n",
      " 128/1000 [==>...........................] - ETA: 7:26 - loss: 8.5926"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-164dcf3ec471>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----- fit model\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "bidirectional_attention_model_history  = bidirectional_attention_model.fit( \n",
    "    batch_size=128,\n",
    "    x=train_data_context, \n",
    "    y=np_utils.to_categorical(train_data_response, num_classes=vocab_size), \n",
    "    validation_data=(test_data_context, np_utils.to_categorical(test_data_response, num_classes=vocab_size)), \n",
    "    epochs=100\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
